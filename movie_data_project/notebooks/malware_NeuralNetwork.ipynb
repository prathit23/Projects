{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.neural_network.multilayer_perceptron module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neural_network. Anything that cannot be imported from sklearn.neural_network is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score, classification_report, make_scorer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import multilayer_perceptron\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "import string\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "filename = \"preprocessed_data.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing data (optional)\n",
    "\n",
    "skip the step if dataset.txt already exists\n",
    "\n",
    "raw data from the Drebin dataset is expected in folder `DATA_DIR`\n",
    "\n",
    "small and medium datasets are contained in this repository. the full dataset can\n",
    "be downloaded from the Drebin webpage\n",
    "\n",
    "in the `DATA_DIR` folder, a file `sha256_family.csv` is expected which indicates the malicious apps\n",
    "and a subdirectory `feature_vectors` is expected which contains a file for every app\n",
    "in the dataset\n",
    "\n",
    "this step reads the raw data and creates a single file in the output path\n",
    "containing the information from all apps\n",
    "\n",
    "the resulting file has two columns separated by tab: first the category of\n",
    "maliciousness (or \"benign\") and second a space-separated list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ckaestne/tmp/sha256_family.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-25ca3616d62f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset generation completed.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-25ca3616d62f>\u001b[0m in \u001b[0;36mgenerate_data\u001b[0;34m(path, targetFile)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# read the list of malicious apps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfamily_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# skip the headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ckaestne/tmp/sha256_family.csv'"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/home/ckaestne/tmp'\n",
    "FAMILY_FILENAME = 'sha256_family.csv'\n",
    "FEATURES = 'feature_vectors'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(path, targetFile):\n",
    "    family_file = os.path.join(path, FAMILY_FILENAME)\n",
    "    feature_path = os.path.join(path, FEATURES)\n",
    "    \n",
    "    # read the list of malicious apps\n",
    "    with open(family_file) as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        next(reader, None)  # skip the headers\n",
    "        malware_dict = {rows[0]: rows[1] for rows in reader}\n",
    "    \n",
    "    # load all the feature files in the DATA_DIR/feature_vectors directory\n",
    "    # write them into single output file\n",
    "    with open(targetFile, mode='w') as dataset:\n",
    "        for file in os.listdir(feature_path):\n",
    "            label = malware_dict.get(file, \"benign\")\n",
    "            feature = extract_feature_naive(os.path.join(feature_path, file))\n",
    "            dataset.write(label + '\\t' + feature + \"\\n\")\n",
    "\n",
    "\n",
    "def extract_feature_naive(filename):\n",
    "    feature = ''\n",
    "    with open(filename, mode='r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            feature = ' '.join([feature, line.replace(\" \", \"_\")])\n",
    "\n",
    "    return feature[1:]\n",
    "\n",
    "\n",
    "generate_data(DATA_DIR, filename)\n",
    "print('Dataset generation completed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data and selecting features\n",
    "\n",
    "the preprocessed data in `filename` contains all the features representing characteristics of the apps.\n",
    "several characteristics include names and URLs, so the space is not limited to a finite set\n",
    "of characteristics.\n",
    "\n",
    "for prediction, it may make sense to select only a subset of these characteristics as features.\n",
    "the code below makes such a selection and translates the data into a big dataframe with one\n",
    "column per selected characteristic. that is, we have a potentially large list of (binary) features.\n",
    "\n",
    "several characteristics are droped (call, activity, url, ...), but could be added back as features\n",
    "again. It's also possible to further preprocess and group characteristics into fewer features or\n",
    "into nonboolean features.\n",
    "\n",
    "the `max_features` parameter of load data restricts the number of features (columns) used\n",
    "to those that occur most frequently across all apps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5600, 1328)\n"
     ]
    }
   ],
   "source": [
    "def feature_selection(x):\n",
    "    # split the long feature string into multiple parts and drop a number of \n",
    "    # extra information that we won't use for modeling (calls, activities, urls)\n",
    "    features = x.split(\" \")\n",
    "    return filter(lambda x: not (x.startswith(\"call\") or x.startswith(\"activity\") or x.startswith(\"url\") or x.startswith(\"provider\") or x.startswith(\"service_receiver\")), features)\n",
    "\n",
    "def load_data(filename, max_features=None):\n",
    "    # create a data frame with pandas\n",
    "    raw_data = pd.read_table(filename, sep='\\t', header=None, names=['label', 'features'])\n",
    "    # convert the labels from strings to binary values\n",
    "    raw_data.label[raw_data.label != 'benign'] = 'malicious'\n",
    "    raw_data = raw_data.dropna()\n",
    "\n",
    "    # transform the data into occurrences,\n",
    "    # which will be the features that we will feed into the model\n",
    "    count_vect = CountVectorizer(analyzer=feature_selection, lowercase=False, max_features=max_features)\n",
    "    counts = count_vect.fit_transform(raw_data['features'])\n",
    "    \n",
    "    # convert resulting matrix into dataframe\n",
    "    df=pd.DataFrame(data=counts.toarray(), columns=count_vect.get_feature_names())\n",
    "    df['label'] = raw_data.label\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "df = load_data(filename)\n",
    "X = df.drop(\"label\", axis=1)\n",
    "y = df[\"label\"]\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple learning code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.99      0.99      0.99       545\n",
      "   malicious       0.63      0.80      0.71        15\n",
      "\n",
      "    accuracy                           0.98       560\n",
      "   macro avg       0.81      0.89      0.85       560\n",
      "weighted avg       0.98      0.98      0.98       560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split train vs test data\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.neural_network import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,\n",
    "                                                    random_state=0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
    "\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(13, 13, 13), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)\n",
    "\n",
    "predictions = mlp.predict(X_test)\n",
    "mlp.score(X_test, y_test)\n",
    "\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "# fit model\n",
    "# model = multilayer_perceptron()\n",
    "# model.fit(X_train,y_train)\n",
    "\n",
    "# # accuracy of the model\n",
    "# model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning with crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=True, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "Precision: 0.49911380487001794\n",
      "Recall: 0.7173160173160172\n",
      "F1 score: 0.586356589296156\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(10, True, 1)\n",
    "s_kfold = StratifiedKFold(10, True, 1)\n",
    "model = MultinomialNB()\n",
    "\n",
    "acc_scores = cross_val_score(model, X, y, cv=s_kfold)\n",
    "print('Accuracy: ' + repr(np.mean(acc_scores)))\n",
    "\n",
    "prec_scores = cross_val_score(model, X, y, scoring=make_scorer(precision_score, pos_label='malicious'), cv=s_kfold)\n",
    "print('Precision: ' + repr(np.mean(prec_scores)))\n",
    "\n",
    "recall_scores = cross_val_score(model, X, y, scoring=make_scorer(recall_score, pos_label='malicious'), cv=s_kfold)\n",
    "print('Recall: ' + repr(np.mean(recall_scores)))\n",
    "\n",
    "f1_scores = cross_val_score(model, X, y, scoring=make_scorer(fbeta_score, beta=1, pos_label='malicious'), cv=s_kfold)\n",
    "print('F1 score: ' + repr(np.mean(f1_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
